# ä»CSVæ–‡ä»¶ä¸­è¯»å–abstractå¹¶embeddingï¼Œå®Œæˆåå­˜faissåº“ (ä½¿ç”¨BCEmbedding)
import os
import pandas as pd
import glob
import faiss
import numpy as np
from typing import List
import time
from sentence_transformers import SentenceTransformer

# === CONFIG ===
CSV_DIR = "/remote-home/jiayuguo/RAG-search/results/"  # CSVæ–‡ä»¶ç›®å½•
CHUNK_SIZE = 1000  # abstracté€šå¸¸è¾ƒçŸ­ï¼Œå¯ä»¥è®¾ç½®smaller chunk size
FAISS_INDEX_PATH = "/remote-home/jiayuguo/RAG-search/abstracts_index.faiss"
METADATA_PATH = "/remote-home/jiayuguo/RAG-search/abstracts_metadata.csv"
BATCH_SIZE = 10  # æ‰¹å¤„ç†å¤§å°

print("ğŸ”§ Configuration loaded.")
print(f"CSV directory: {CSV_DIR}")
print(f"Chunk size: {CHUNK_SIZE}")
print(f"Batch size: {BATCH_SIZE}\n")

# === INIT EMBEDDING MODEL ===
model = SentenceTransformer("maidalun1020/bce-embedding-base_v1")
embedding_dim = model.get_sentence_embedding_dimension()
print(f"âœ… BCEmbedding model loaded. Dimension: {embedding_dim}")

# === FUNCTIONS ===
def extract_abstracts_from_csv(csv_path: str) -> List[dict]:
    try:
        df = pd.read_csv(csv_path)
        abstracts_data = []

        for idx, row in df.iterrows():
            abstract = str(row.get('Abstract', '')).strip()
            if abstract and abstract != 'nan' and len(abstract) > 50:
                abstracts_data.append({
                    'pmid': row.get('PMID', ''),
                    'title': row.get('Title', ''),
                    'abstract': abstract,
                    'journal': row.get('Journal', ''),
                    'date': row.get('Date', ''),
                    'source_file': os.path.basename(csv_path)
                })

        print(f"âœ… ä» {csv_path} æå–äº† {len(abstracts_data)} ä¸ªæœ‰æ•ˆabstract")
        return abstracts_data

    except Exception as e:
        print(f"âŒ Error reading {csv_path}: {e}")
        return []

def chunk_text(text: str, max_length: int = 1000) -> List[str]:
    if len(text) <= max_length:
        return [text]
    chunks = []
    while len(text) > max_length:
        split_idx = text.rfind('.', 0, max_length)
        if split_idx == -1:
            split_idx = max_length
        chunks.append(text[:split_idx+1].strip())
        text = text[split_idx+1:].strip()
    if text:
        chunks.append(text)
    return chunks

# === FIND ALL CSV FILES ===
print("\nğŸ“„ Finding CSV files...")
csv_files = glob.glob(os.path.join(CSV_DIR, "*.csv"))
print(f"âœ… Found {len(csv_files)} CSV files.")

if not csv_files:
    print("âŒ No CSV files found in results directory!")
    exit(1)

# === INIT FAISS ===
index = faiss.IndexFlatL2(embedding_dim)
metadata = []

# === PROCESS CSV FILES ===
total_abstracts = 0
for csv_file in csv_files:
    print(f"\nğŸ“„ Processing CSV: {csv_file}")
    abstracts_data = extract_abstracts_from_csv(csv_file)

    if not abstracts_data:
        print("âš ï¸ No valid abstracts found. Skipping.")
        continue

    texts_to_embed = []
    current_metadata = []

    for data in abstracts_data:
        abstract = data['abstract']
        chunks = chunk_text(abstract, max_length=CHUNK_SIZE)

        for i, chunk in enumerate(chunks):
            texts_to_embed.append(chunk)
            current_metadata.append({
                'pmid': data['pmid'],
                'title': data['title'],
                'text': chunk,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'journal': data['journal'],
                'date': data['date'],
                'source_file': data['source_file']
            })

    if not texts_to_embed:
        continue

    print(f"âœ‚ï¸  Created {len(texts_to_embed)} text chunks for embedding.")

    try:
        all_embeddings = []
        for i in range(0, len(texts_to_embed), BATCH_SIZE):
            batch_texts = texts_to_embed[i:i+BATCH_SIZE]
            batch_num = i//BATCH_SIZE + 1
            total_batches = (len(texts_to_embed) + BATCH_SIZE - 1)//BATCH_SIZE
            print(f"ğŸ“¦ Processing batch {batch_num}/{total_batches}")

            batch_embeddings = model.encode(batch_texts)
            all_embeddings.extend(batch_embeddings.tolist())
            print(f"âœ… Batch {batch_num} completed. Total embeddings so far: {len(all_embeddings)}")

        if len(all_embeddings) != len(texts_to_embed):
            print(f"âš ï¸ Warning: Expected {len(texts_to_embed)} embeddings, got {len(all_embeddings)}")

        if all_embeddings:
            embeddings_array = np.array(all_embeddings).astype("float32")
            index.add(embeddings_array)
            metadata.extend(current_metadata[:len(all_embeddings)])
            total_abstracts += len(abstracts_data)
            print(f"ğŸ“Œ Added {len(all_embeddings)} embeddings to FAISS.")

    except Exception as e:
        print(f"âŒ Embedding failed for {csv_file}: {e}")
        continue

print(f"\nğŸ‰ Total processed: {total_abstracts} abstracts from {len(csv_files)} files")
print(f"ğŸ“Š Total chunks in FAISS index: {len(metadata)}")

# === SAVE FAISS INDEX AND METADATA ===
print("\nğŸ’¾ Saving FAISS index and metadata...")
faiss.write_index(index, FAISS_INDEX_PATH)

metadata_df = pd.DataFrame(metadata)
metadata_df.to_csv(
    METADATA_PATH,
    index=False,
    escapechar='\\',
    quoting=1
)

print(f"\nâœ… FAISS index saved to: {FAISS_INDEX_PATH}")
print(f"âœ… Metadata saved to: {METADATA_PATH}")
print(f"ğŸ“ˆ Index contains {index.ntotal} vectors")
